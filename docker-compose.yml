services:
  server:
    build:
      args:
        HF_TOKEN: $HF_TOKEN
        MAX_MODEL_LENGTH: "2048"
        MODEL: "facebook/opt-125m"
      context: .
    container_name: llm-server
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              count: all
              driver: nvidia
    environment:
      HOST: "0.0.0.0"
      PORT: "8000"
      TENSOR_PARALLEL_SIZE: "1"
    image: ivangabriele/llm:facebook__opt-125m
    ipc: host
    logging:
      driver: json-file
      options:
        max-file: "1"
        max-size: "10m"
    ports:
      - "22:22"
      - "8000:8000"
    shm_size: 1024M
    ulimits:
      stack: 67108864
